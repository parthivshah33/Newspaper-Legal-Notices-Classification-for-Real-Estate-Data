{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8503841,"sourceType":"datasetVersion","datasetId":5075431}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# TPU Library Installation\n# !pip install torch-xla\n\n!pip install torchviz\n!pip install torchview\n!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:06.257287Z","iopub.execute_input":"2024-05-31T06:02:06.257629Z","iopub.status.idle":"2024-05-31T06:02:45.608551Z","shell.execute_reply.started":"2024-05-31T06:02:06.257602Z","shell.execute_reply":"2024-05-31T06:02:45.607323Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torchviz\n  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchviz) (2.1.2)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from torchviz) (0.20.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (2024.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchviz) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchviz) (1.3.0)\nBuilding wheels for collected packages: torchviz\n  Building wheel for torchviz (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=2cdf479a39352a849a6287e72927f191056c80316731051b188fdab8af467163\n  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\nSuccessfully built torchviz\nInstalling collected packages: torchviz\nSuccessfully installed torchviz-0.0.2\nCollecting torchview\n  Downloading torchview-0.2.6-py3-none-any.whl.metadata (12 kB)\nDownloading torchview-0.2.6-py3-none-any.whl (25 kB)\nInstalling collected packages: torchview\nSuccessfully installed torchview-0.2.6\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Continue with regular imports\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\n\ntry : \n    \n    # imports the torch_xla package\n    import torch_xla\n    import torch_xla.core.xla_model as xm\nexcept : \n    pass\n\nfrom PIL import Image\nfrom torch import nn\nfrom torchvision import transforms,datasets\nfrom torch.utils.data import DataLoader\n\n# to visualize model Architecture\nfrom torchview import draw_graph\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    %pip install -q torchinfo\n    from torchinfo import summary\nCUDA_LAUNCH_BLOCKING=1\n\ntorch.cuda.empty_cache()\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:45.610829Z","iopub.execute_input":"2024-05-31T06:02:45.611227Z","iopub.status.idle":"2024-05-31T06:02:51.574908Z","shell.execute_reply.started":"2024-05-31T06:02:45.611193Z","shell.execute_reply":"2024-05-31T06:02:51.573930Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dir = \"/kaggle/input/jaher-notice-4th-time-dataset-with-1917-train-img/Jaher Notice full dataset/Train Data\"\ntest_dir = \"/kaggle/input/jaher-notice-4th-time-dataset-with-1917-train-img/Jaher Notice full dataset/Test Data\"","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:51.575972Z","iopub.execute_input":"2024-05-31T06:02:51.576362Z","iopub.status.idle":"2024-05-31T06:02:51.580455Z","shell.execute_reply.started":"2024-05-31T06:02:51.576336Z","shell.execute_reply":"2024-05-31T06:02:51.579413Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"NUM_WORKERS = 8\n\ndef create_dataloaders(\n    train_dir: str, \n    test_dir: str, \n    transform: transforms.Compose, \n    batch_size: int, \n    num_workers: int=NUM_WORKERS\n):\n  \n  # Use ImageFolder to create dataset(s)\n  train_data = datasets.ImageFolder(train_dir, transform=transform)\n  test_data = datasets.ImageFolder(test_dir, transform=transform)\n\n  # Get class names\n  class_names = train_data.classes\n\n  # Turn images into data loaders\n  train_dataloader = DataLoader(\n      train_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n  test_dataloader = DataLoader(\n      test_data,\n      batch_size=batch_size,\n      shuffle=False, # don't need to shuffle test data\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n\n  return train_dataloader, test_dataloader, class_names","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:51.583042Z","iopub.execute_input":"2024-05-31T06:02:51.583499Z","iopub.status.idle":"2024-05-31T06:02:51.592650Z","shell.execute_reply.started":"2024-05-31T06:02:51.583468Z","shell.execute_reply":"2024-05-31T06:02:51.591874Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Auto `transforms`","metadata":{}},{"cell_type":"code","source":"# Get a set of pretrained model weights\nweights = torchvision.models.EfficientNet_B4_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\nweights","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:51.593792Z","iopub.execute_input":"2024-05-31T06:02:51.594137Z","iopub.status.idle":"2024-05-31T06:02:51.608243Z","shell.execute_reply.started":"2024-05-31T06:02:51.594082Z","shell.execute_reply":"2024-05-31T06:02:51.607496Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"EfficientNet_B4_Weights.IMAGENET1K_V1"},"metadata":{}}]},{"cell_type":"code","source":"# Get the transforms used to create our pretrained weights\nauto_transforms = weights.transforms()\nauto_transforms","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:51.609162Z","iopub.execute_input":"2024-05-31T06:02:51.609436Z","iopub.status.idle":"2024-05-31T06:02:51.618978Z","shell.execute_reply.started":"2024-05-31T06:02:51.609404Z","shell.execute_reply":"2024-05-31T06:02:51.618162Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"ImageClassification(\n    crop_size=[380]\n    resize_size=[384]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)"},"metadata":{}}]},{"cell_type":"code","source":"# This Technique is not worked very well (31-May-2024)\n\n# # Define additional augmentation transformations\n# color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n\n# # Combine both transformations\n# customized_transforms = transforms.Compose([\n#     auto_transforms,\n#     color_jitter,\n# ])","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:51.620094Z","iopub.execute_input":"2024-05-31T06:02:51.620393Z","iopub.status.idle":"2024-05-31T06:02:51.627006Z","shell.execute_reply.started":"2024-05-31T06:02:51.620371Z","shell.execute_reply":"2024-05-31T06:02:51.626248Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:51.628037Z","iopub.execute_input":"2024-05-31T06:02:51.628327Z","iopub.status.idle":"2024-05-31T06:02:53.394411Z","shell.execute_reply.started":"2024-05-31T06:02:51.628305Z","shell.execute_reply":"2024-05-31T06:02:53.393546Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(<torch.utils.data.dataloader.DataLoader at 0x7bc07ed14e50>,\n <torch.utils.data.dataloader.DataLoader at 0x7bc07ed15d50>,\n ['JN', 'dump'])"},"metadata":{}}]},{"cell_type":"code","source":"# choose this to train on new dataset\nmodel = torchvision.models.efficientnet_b4(weights=weights).to(device)\n\n# choose this to finetune on new similar data that it has already been trained on.\n# model = torch.load('/kaggle/input/jaher_notice_segregator_base_model_pytorch/pytorch/base_model/1/full_model_model_train_loss_ 0.0460 _ train_acc_ 0.5326 _ test_loss_ 84.5817 _ test_acc_ 0.9679.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:53.395765Z","iopub.execute_input":"2024-05-31T06:02:53.396224Z","iopub.status.idle":"2024-05-31T06:02:55.685959Z","shell.execute_reply.started":"2024-05-31T06:02:53.396190Z","shell.execute_reply":"2024-05-31T06:02:55.685149Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n100%|██████████| 74.5M/74.5M [00:01<00:00, 53.9MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# *freezing only pre-trained model parameters and allowing externally added classification layer weights to be updated*","metadata":{}},{"cell_type":"code","source":"for name,param in model.features[:8].named_parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:55.688647Z","iopub.execute_input":"2024-05-31T06:02:55.688915Z","iopub.status.idle":"2024-05-31T06:02:55.695840Z","shell.execute_reply.started":"2024-05-31T06:02:55.688892Z","shell.execute_reply":"2024-05-31T06:02:55.694991Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# for name,param in model.features.named_parameters():\n#     print(name,\",        req_grad = \", param.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:55.696913Z","iopub.execute_input":"2024-05-31T06:02:55.697503Z","iopub.status.idle":"2024-05-31T06:02:55.706325Z","shell.execute_reply.started":"2024-05-31T06:02:55.697478Z","shell.execute_reply":"2024-05-31T06:02:55.705424Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n# for param in model.parameters():\n#     param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:55.707537Z","iopub.execute_input":"2024-05-31T06:02:55.707942Z","iopub.status.idle":"2024-05-31T06:02:55.715956Z","shell.execute_reply.started":"2024-05-31T06:02:55.707912Z","shell.execute_reply":"2024-05-31T06:02:55.715140Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# # Allow All the classifier layer parameter to be updated during training\n# for name,param in model.classifier.named_parameters():\n# #     print(name)\n#     param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:55.716941Z","iopub.execute_input":"2024-05-31T06:02:55.717241Z","iopub.status.idle":"2024-05-31T06:02:55.724987Z","shell.execute_reply.started":"2024-05-31T06:02:55.717219Z","shell.execute_reply":"2024-05-31T06:02:55.724294Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# EXPERIMENTAL CLASSIFIER ARCHITECTURE\n# (added addtional 64 node dense layer after 128 node dense layer with ReLu Activation Function)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n# \"He/Kaiming Normal\" Weight Initialization\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n\n# Assuming 'device' is already defined\n\n# Recreate the classifier layer and seed it to the target device\n''' normalization added after every linear layer. \"64,32,16 neuron dense layer also added.\",\n\"dropout layers also added, He weight initializer also added.\"\n'''\n\nmodel.classifier = nn.Sequential(\n    nn.Linear(in_features=1792, out_features=1024),\n    nn.BatchNorm1d(1024),\n    nn.ReLU(),\n    nn.Dropout(p=0.2),  # Dropout added after the first linear layer\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),\n    nn.ReLU(),\n    nn.Dropout(p=0.3),  # Dropout added after the second linear layer\n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),\n    nn.ReLU(),\n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),\n    nn.ReLU(),\n    nn.Dropout(p=0.4),  # Dropout added after the third linear layer\n    nn.Linear(128, 64),\n    nn.BatchNorm1d(64),\n    nn.ReLU(),\n    nn.Linear(in_features=64, out_features=1, bias=True)  # Final output layer\n).to(device)\n\n# Apply He initialization to all linear layers\nmodel.apply(init_weights)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:55.726017Z","iopub.execute_input":"2024-05-31T06:02:55.726526Z","iopub.status.idle":"2024-05-31T06:02:55.784509Z","shell.execute_reply.started":"2024-05-31T06:02:55.726501Z","shell.execute_reply":"2024-05-31T06:02:55.783681Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"EfficientNet(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.00625, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.018750000000000003, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.03125, mode=row)\n      )\n    )\n    (3): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.043750000000000004, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.05625, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(336, 336, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=336, bias=False)\n            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.06875, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.08125, mode=row)\n      )\n      (4): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n      )\n      (5): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.09375, mode=row)\n      )\n    )\n    (5): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.10625000000000001, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.11875000000000001, mode=row)\n      )\n      (4): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n      )\n      (5): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.13125, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.14375000000000002, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.15625, mode=row)\n      )\n      (4): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n      )\n      (5): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.16875, mode=row)\n      )\n      (6): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n      )\n      (7): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.18125000000000002, mode=row)\n      )\n    )\n    (7): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n            (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(2688, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(2688, 2688, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2688, bias=False)\n            (1): BatchNorm2d(2688, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(2688, 112, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(112, 2688, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.19375, mode=row)\n      )\n    )\n    (8): Conv2dNormActivation(\n      (0): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): Linear(in_features=1792, out_features=1024, bias=True)\n    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.2, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=True)\n    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Dropout(p=0.3, inplace=False)\n    (8): Linear(in_features=512, out_features=256, bias=True)\n    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n    (11): Linear(in_features=256, out_features=128, bias=True)\n    (12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (13): ReLU()\n    (14): Dropout(p=0.4, inplace=False)\n    (15): Linear(in_features=128, out_features=64, bias=True)\n    (16): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (17): ReLU()\n    (18): Linear(in_features=64, out_features=1, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Define loss and optimizer\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:55.785557Z","iopub.execute_input":"2024-05-31T06:02:55.785805Z","iopub.status.idle":"2024-05-31T06:02:55.793029Z","shell.execute_reply.started":"2024-05-31T06:02:55.785784Z","shell.execute_reply":"2024-05-31T06:02:55.792052Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n  \n  # Put model in train mode\n  model.train()\n\n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n\n  # Loop through data loader data batches\n  for batch, (X, y) in enumerate(dataloader):\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n      # y_pred = torch.sigmoid(model(X))\n      # 2. Calculate  and accumulate loss\n      y = y.unsqueeze(1).float()\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      # y_pred_class = y_pred.round()\n      # train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n      # train_acc += (y_pred_class.eq(y).float().sum().item() / len(y))\n      train_acc += ((torch.round(torch.sigmoid(y_pred))).eq(y)).float().mean().item()\n      # print(f\"Batch: {batch}, Predicted: {y_pred_class}, y_train: {y}\")\n\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -> Tuple[float, float]:\n  \n  # Put model in eval mode\n  model.eval() \n\n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n\n  # Turn on inference context manager\n  with torch.inference_mode():\n      # Loop through DataLoader batches\n      for batch, (X, y) in enumerate(dataloader):\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n\n          # 1. Forward pass\n          # test_pred_logits = torch.sigmoid(model(X))\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          y = y.unsqueeze(1).float()\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n\n          # Calculate and accumulate accuracy\n          # test_pred_labels = test_pred_logits.round()\n          # test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n          # print((test_pred_logits > 0.5).eq(y).sum())\n          test_acc += ((torch.round(torch.sigmoid(test_pred_logits))).eq(y).float().mean().item())\n\n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -> Dict[str, List]:\n  \n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n  }\n\n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs)):\n      train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n      test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n      # Print out what's happening\n      print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n      )\n\n      # Update results dictionary\n      results[\"train_loss\"].append(train_loss)\n      results[\"train_acc\"].append(train_acc)\n      results[\"test_loss\"].append(test_loss)\n      results[\"test_acc\"].append(test_acc)\n\n  # Return the filled results at the end of the epochs\n  return results","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:55.794364Z","iopub.execute_input":"2024-05-31T06:02:55.794635Z","iopub.status.idle":"2024-05-31T06:02:55.813091Z","shell.execute_reply.started":"2024-05-31T06:02:55.794613Z","shell.execute_reply":"2024-05-31T06:02:55.812313Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Set the random seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Setup training and save the results\nresults =   train(model=model,\n                    train_dataloader=train_dataloader,\n                    test_dataloader=test_dataloader,\n                    optimizer=optimizer,\n                    loss_fn=loss_fn,\n                    epochs=369,\n                    device=device)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:02:55.814180Z","iopub.execute_input":"2024-05-31T06:02:55.814447Z","iopub.status.idle":"2024-05-31T10:51:56.783313Z","shell.execute_reply.started":"2024-05-31T06:02:55.814425Z","shell.execute_reply":"2024-05-31T10:51:56.782184Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/369 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd460ba559c54313aa7cecedd31f1e8f"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 | train_loss: 1.8713 | train_acc: 0.6245 | test_loss: 131.4703 | test_acc: 0.0000\nEpoch: 2 | train_loss: 1.0261 | train_acc: 0.7409 | test_loss: 2.1733 | test_acc: 0.0000\nEpoch: 3 | train_loss: 0.8268 | train_acc: 0.7867 | test_loss: 141520.0156 | test_acc: 0.0000\nEpoch: 4 | train_loss: 0.6587 | train_acc: 0.8164 | test_loss: 557.2786 | test_acc: 0.0000\nEpoch: 5 | train_loss: 0.6363 | train_acc: 0.8257 | test_loss: 25001.3359 | test_acc: 0.0000\nEpoch: 6 | train_loss: 0.5832 | train_acc: 0.8395 | test_loss: 832493.7500 | test_acc: 0.0000\nEpoch: 7 | train_loss: 0.5695 | train_acc: 0.8376 | test_loss: 86360.6172 | test_acc: 0.0000\nEpoch: 8 | train_loss: 0.4967 | train_acc: 0.8433 | test_loss: 25699.2207 | test_acc: 0.0000\nEpoch: 9 | train_loss: 0.4291 | train_acc: 0.8608 | test_loss: 72096.7656 | test_acc: 0.0000\nEpoch: 10 | train_loss: 0.4211 | train_acc: 0.8672 | test_loss: 169772.0938 | test_acc: 0.0000\nEpoch: 11 | train_loss: 0.3828 | train_acc: 0.8781 | test_loss: 76524.3828 | test_acc: 0.0000\nEpoch: 12 | train_loss: 0.3748 | train_acc: 0.8779 | test_loss: 316341.3438 | test_acc: 0.0000\nEpoch: 13 | train_loss: 0.3366 | train_acc: 0.8890 | test_loss: 59879.9922 | test_acc: 0.0000\nEpoch: 14 | train_loss: 0.3588 | train_acc: 0.8785 | test_loss: 128806.2500 | test_acc: 0.0000\nEpoch: 15 | train_loss: 0.3666 | train_acc: 0.8812 | test_loss: 0.7401 | test_acc: 0.5000\nEpoch: 16 | train_loss: 0.3078 | train_acc: 0.8932 | test_loss: 20401.2402 | test_acc: 0.0000\nEpoch: 17 | train_loss: 0.3263 | train_acc: 0.8860 | test_loss: 175890.3125 | test_acc: 0.0000\nEpoch: 18 | train_loss: 0.3089 | train_acc: 0.8925 | test_loss: 2009.1106 | test_acc: 0.0000\nEpoch: 19 | train_loss: 0.2706 | train_acc: 0.9026 | test_loss: 169907.6719 | test_acc: 0.0000\nEpoch: 20 | train_loss: 0.2465 | train_acc: 0.9129 | test_loss: 145128.0312 | test_acc: 0.0000\nEpoch: 21 | train_loss: 0.2711 | train_acc: 0.9014 | test_loss: 47399.0977 | test_acc: 0.0000\nEpoch: 22 | train_loss: 0.2660 | train_acc: 0.9083 | test_loss: 83708.3672 | test_acc: 0.0000\nEpoch: 23 | train_loss: 0.2604 | train_acc: 0.9109 | test_loss: 144265.3125 | test_acc: 0.0000\nEpoch: 24 | train_loss: 0.2172 | train_acc: 0.9190 | test_loss: 174451.2344 | test_acc: 0.0000\nEpoch: 25 | train_loss: 0.2215 | train_acc: 0.9148 | test_loss: 38117.7422 | test_acc: 0.0000\nEpoch: 26 | train_loss: 0.2164 | train_acc: 0.9197 | test_loss: 51551.2812 | test_acc: 0.0000\nEpoch: 27 | train_loss: 0.2078 | train_acc: 0.9201 | test_loss: 20390.7422 | test_acc: 0.0000\nEpoch: 28 | train_loss: 0.2061 | train_acc: 0.9268 | test_loss: 144876.8281 | test_acc: 0.0000\nEpoch: 29 | train_loss: 0.1913 | train_acc: 0.9244 | test_loss: 120349.7656 | test_acc: 0.0000\nEpoch: 30 | train_loss: 0.1928 | train_acc: 0.9294 | test_loss: 116201.9219 | test_acc: 0.0000\nEpoch: 31 | train_loss: 0.1858 | train_acc: 0.9273 | test_loss: 34601.9727 | test_acc: 0.0000\nEpoch: 32 | train_loss: 0.1791 | train_acc: 0.9332 | test_loss: 153823.6719 | test_acc: 0.0000\nEpoch: 33 | train_loss: 0.1710 | train_acc: 0.9343 | test_loss: 316090.4062 | test_acc: 0.0000\nEpoch: 34 | train_loss: 0.1530 | train_acc: 0.9414 | test_loss: 143246.1094 | test_acc: 0.0000\nEpoch: 35 | train_loss: 0.1696 | train_acc: 0.9330 | test_loss: 37215.3633 | test_acc: 0.0000\nEpoch: 36 | train_loss: 0.1718 | train_acc: 0.9371 | test_loss: 14460.4727 | test_acc: 0.0000\nEpoch: 37 | train_loss: 0.1579 | train_acc: 0.9391 | test_loss: 8489.5449 | test_acc: 0.0000\nEpoch: 38 | train_loss: 0.1560 | train_acc: 0.9369 | test_loss: 13178.6914 | test_acc: 0.0000\nEpoch: 39 | train_loss: 0.1332 | train_acc: 0.9481 | test_loss: 129713.2422 | test_acc: 0.0000\nEpoch: 40 | train_loss: 0.1358 | train_acc: 0.9474 | test_loss: 76470.9062 | test_acc: 0.0000\nEpoch: 41 | train_loss: 0.1307 | train_acc: 0.9498 | test_loss: 71820.8906 | test_acc: 0.0000\nEpoch: 42 | train_loss: 0.1551 | train_acc: 0.9404 | test_loss: 10530.3516 | test_acc: 0.0000\nEpoch: 43 | train_loss: 0.1313 | train_acc: 0.9470 | test_loss: 55719.8750 | test_acc: 0.0000\nEpoch: 44 | train_loss: 0.1300 | train_acc: 0.9515 | test_loss: 0.9090 | test_acc: 0.5000\nEpoch: 45 | train_loss: 0.1294 | train_acc: 0.9551 | test_loss: 1.3626 | test_acc: 0.0000\nEpoch: 46 | train_loss: 0.1315 | train_acc: 0.9542 | test_loss: 1.2498 | test_acc: 0.5000\nEpoch: 47 | train_loss: 0.1297 | train_acc: 0.9486 | test_loss: 37315.2461 | test_acc: 0.0000\nEpoch: 48 | train_loss: 0.1193 | train_acc: 0.9554 | test_loss: 20964.7695 | test_acc: 0.0000\nEpoch: 49 | train_loss: 0.1211 | train_acc: 0.9522 | test_loss: 1.4545 | test_acc: 0.5000\nEpoch: 50 | train_loss: 0.1164 | train_acc: 0.9536 | test_loss: 181.0777 | test_acc: 0.0000\nEpoch: 51 | train_loss: 0.1038 | train_acc: 0.9623 | test_loss: 266635.6875 | test_acc: 0.0000\nEpoch: 52 | train_loss: 0.1052 | train_acc: 0.9576 | test_loss: 49430.5078 | test_acc: 0.0000\nEpoch: 53 | train_loss: 0.1069 | train_acc: 0.9609 | test_loss: 12936.6641 | test_acc: 0.0000\nEpoch: 54 | train_loss: 0.1154 | train_acc: 0.9583 | test_loss: 1.4043 | test_acc: 0.5000\nEpoch: 55 | train_loss: 0.0930 | train_acc: 0.9617 | test_loss: 87946.5234 | test_acc: 0.0000\nEpoch: 56 | train_loss: 0.0942 | train_acc: 0.9639 | test_loss: 159142.2344 | test_acc: 0.0000\nEpoch: 57 | train_loss: 0.0910 | train_acc: 0.9648 | test_loss: 57531.6367 | test_acc: 0.0000\nEpoch: 58 | train_loss: 0.1000 | train_acc: 0.9627 | test_loss: 123017.6406 | test_acc: 0.0000\nEpoch: 59 | train_loss: 0.0951 | train_acc: 0.9672 | test_loss: 65516.5078 | test_acc: 0.0000\nEpoch: 60 | train_loss: 0.1033 | train_acc: 0.9619 | test_loss: 59414.3203 | test_acc: 0.0000\nEpoch: 61 | train_loss: 0.0816 | train_acc: 0.9676 | test_loss: 152307.4531 | test_acc: 0.0000\nEpoch: 62 | train_loss: 0.0874 | train_acc: 0.9708 | test_loss: 89287.1797 | test_acc: 0.0000\nEpoch: 63 | train_loss: 0.0903 | train_acc: 0.9677 | test_loss: 1.9476 | test_acc: 0.5000\nEpoch: 64 | train_loss: 0.0732 | train_acc: 0.9721 | test_loss: 60635.1914 | test_acc: 0.0000\nEpoch: 65 | train_loss: 0.0782 | train_acc: 0.9709 | test_loss: 22852.6973 | test_acc: 0.0000\nEpoch: 66 | train_loss: 0.0872 | train_acc: 0.9693 | test_loss: 125135.1875 | test_acc: 0.0000\nEpoch: 67 | train_loss: 0.0835 | train_acc: 0.9673 | test_loss: 144215.9062 | test_acc: 0.0000\nEpoch: 68 | train_loss: 0.0753 | train_acc: 0.9718 | test_loss: 1.6480 | test_acc: 0.5000\nEpoch: 69 | train_loss: 0.0797 | train_acc: 0.9695 | test_loss: 16760.5000 | test_acc: 0.0000\nEpoch: 70 | train_loss: 0.0602 | train_acc: 0.9763 | test_loss: 13825.2178 | test_acc: 0.0000\nEpoch: 71 | train_loss: 0.0721 | train_acc: 0.9716 | test_loss: 74536.4219 | test_acc: 0.0000\nEpoch: 72 | train_loss: 0.0672 | train_acc: 0.9757 | test_loss: 5622.8418 | test_acc: 0.0000\nEpoch: 73 | train_loss: 0.0573 | train_acc: 0.9807 | test_loss: 1487.8031 | test_acc: 0.0000\nEpoch: 74 | train_loss: 0.0633 | train_acc: 0.9752 | test_loss: 2.3642 | test_acc: 0.5000\nEpoch: 75 | train_loss: 0.0714 | train_acc: 0.9739 | test_loss: 2.6526 | test_acc: 0.5000\nEpoch: 76 | train_loss: 0.0659 | train_acc: 0.9744 | test_loss: 16939.2344 | test_acc: 0.0000\nEpoch: 77 | train_loss: 0.0670 | train_acc: 0.9760 | test_loss: 26076.2793 | test_acc: 0.0000\nEpoch: 78 | train_loss: 0.0609 | train_acc: 0.9791 | test_loss: 105778.0234 | test_acc: 0.0000\nEpoch: 79 | train_loss: 0.0670 | train_acc: 0.9760 | test_loss: 300577.8750 | test_acc: 0.0000\nEpoch: 80 | train_loss: 0.0709 | train_acc: 0.9753 | test_loss: 9334.9150 | test_acc: 0.0000\nEpoch: 81 | train_loss: 0.0616 | train_acc: 0.9755 | test_loss: 20815.6289 | test_acc: 0.0000\nEpoch: 82 | train_loss: 0.0739 | train_acc: 0.9746 | test_loss: 11692.0693 | test_acc: 0.0000\nEpoch: 83 | train_loss: 0.0475 | train_acc: 0.9828 | test_loss: 76132.2188 | test_acc: 0.0000\nEpoch: 84 | train_loss: 0.0613 | train_acc: 0.9809 | test_loss: 95543.3594 | test_acc: 0.0000\nEpoch: 85 | train_loss: 0.0515 | train_acc: 0.9799 | test_loss: 51404.9258 | test_acc: 0.0000\nEpoch: 86 | train_loss: 0.0476 | train_acc: 0.9833 | test_loss: 49293.1641 | test_acc: 0.0000\nEpoch: 87 | train_loss: 0.0653 | train_acc: 0.9789 | test_loss: 76640.8047 | test_acc: 0.0000\nEpoch: 88 | train_loss: 0.0546 | train_acc: 0.9789 | test_loss: 2.3545 | test_acc: 0.5000\nEpoch: 89 | train_loss: 0.0424 | train_acc: 0.9841 | test_loss: 90732.0859 | test_acc: 0.0000\nEpoch: 91 | train_loss: 0.0402 | train_acc: 0.9849 | test_loss: 175077.6875 | test_acc: 0.0000\nEpoch: 92 | train_loss: 0.0453 | train_acc: 0.9833 | test_loss: 11258.6826 | test_acc: 0.0000\nEpoch: 93 | train_loss: 0.0458 | train_acc: 0.9859 | test_loss: 11994.5967 | test_acc: 0.0000\nEpoch: 94 | train_loss: 0.0390 | train_acc: 0.9856 | test_loss: 2954.5269 | test_acc: 0.0000\nEpoch: 95 | train_loss: 0.0512 | train_acc: 0.9828 | test_loss: 156016.2031 | test_acc: 0.0000\nEpoch: 96 | train_loss: 0.0643 | train_acc: 0.9786 | test_loss: 332150.9062 | test_acc: 0.0000\nEpoch: 97 | train_loss: 0.0453 | train_acc: 0.9831 | test_loss: 150854.2812 | test_acc: 0.0000\nEpoch: 98 | train_loss: 0.0360 | train_acc: 0.9880 | test_loss: 101304.6875 | test_acc: 0.0000\nEpoch: 99 | train_loss: 0.0503 | train_acc: 0.9828 | test_loss: 157906.8125 | test_acc: 0.0000\nEpoch: 100 | train_loss: 0.0414 | train_acc: 0.9844 | test_loss: 37271.2188 | test_acc: 0.0000\nEpoch: 101 | train_loss: 0.0490 | train_acc: 0.9827 | test_loss: 57179.5312 | test_acc: 0.0000\nEpoch: 102 | train_loss: 0.0364 | train_acc: 0.9874 | test_loss: 100438.8203 | test_acc: 0.0000\nEpoch: 103 | train_loss: 0.0337 | train_acc: 0.9883 | test_loss: 2.8677 | test_acc: 0.5000\nEpoch: 104 | train_loss: 0.0409 | train_acc: 0.9857 | test_loss: 3.1674 | test_acc: 0.5000\nEpoch: 105 | train_loss: 0.0391 | train_acc: 0.9862 | test_loss: 26199.8984 | test_acc: 0.0000\nEpoch: 106 | train_loss: 0.0401 | train_acc: 0.9843 | test_loss: 110000.6797 | test_acc: 0.0000\nEpoch: 107 | train_loss: 0.0385 | train_acc: 0.9848 | test_loss: 73187.3516 | test_acc: 0.0000\nEpoch: 108 | train_loss: 0.0396 | train_acc: 0.9862 | test_loss: 248138.1562 | test_acc: 0.0000\nEpoch: 109 | train_loss: 0.0342 | train_acc: 0.9898 | test_loss: 27328.1582 | test_acc: 0.0000\nEpoch: 110 | train_loss: 0.0394 | train_acc: 0.9849 | test_loss: 146.0972 | test_acc: 0.0000\nEpoch: 111 | train_loss: 0.0403 | train_acc: 0.9859 | test_loss: 144118.4219 | test_acc: 0.0000\nEpoch: 112 | train_loss: 0.0311 | train_acc: 0.9880 | test_loss: 111081.5781 | test_acc: 0.0000\nEpoch: 113 | train_loss: 0.0356 | train_acc: 0.9865 | test_loss: 3843.8667 | test_acc: 0.0000\nEpoch: 114 | train_loss: 0.0385 | train_acc: 0.9870 | test_loss: 13731.6445 | test_acc: 0.0000\nEpoch: 115 | train_loss: 0.0339 | train_acc: 0.9883 | test_loss: 56187.8750 | test_acc: 0.0000\nEpoch: 116 | train_loss: 0.0488 | train_acc: 0.9857 | test_loss: 139.3594 | test_acc: 0.0000\nEpoch: 117 | train_loss: 0.0392 | train_acc: 0.9872 | test_loss: 38273.3281 | test_acc: 0.0000\nEpoch: 118 | train_loss: 0.0366 | train_acc: 0.9867 | test_loss: 21435.0957 | test_acc: 0.0000\nEpoch: 119 | train_loss: 0.0425 | train_acc: 0.9854 | test_loss: 42322.5234 | test_acc: 0.0000\nEpoch: 120 | train_loss: 0.0376 | train_acc: 0.9882 | test_loss: 2.8856 | test_acc: 0.5000\nEpoch: 121 | train_loss: 0.0281 | train_acc: 0.9909 | test_loss: 5244.4834 | test_acc: 0.0000\nEpoch: 122 | train_loss: 0.0277 | train_acc: 0.9914 | test_loss: 3.4061 | test_acc: 0.5000\nEpoch: 123 | train_loss: 0.0335 | train_acc: 0.9877 | test_loss: 2.9316 | test_acc: 0.5000\nEpoch: 124 | train_loss: 0.0317 | train_acc: 0.9898 | test_loss: 225196.8906 | test_acc: 0.0000\nEpoch: 125 | train_loss: 0.0261 | train_acc: 0.9914 | test_loss: 103010.5156 | test_acc: 0.0000\nEpoch: 126 | train_loss: 0.0339 | train_acc: 0.9867 | test_loss: 137539.5000 | test_acc: 0.0000\nEpoch: 127 | train_loss: 0.0318 | train_acc: 0.9864 | test_loss: 194626.1719 | test_acc: 0.0000\nEpoch: 128 | train_loss: 0.0243 | train_acc: 0.9914 | test_loss: 103027.0547 | test_acc: 0.0000\nEpoch: 129 | train_loss: 0.0323 | train_acc: 0.9891 | test_loss: 148535.1562 | test_acc: 0.0000\nEpoch: 130 | train_loss: 0.0257 | train_acc: 0.9922 | test_loss: 8819.6523 | test_acc: 0.0000\nEpoch: 131 | train_loss: 0.0254 | train_acc: 0.9924 | test_loss: 17983.1191 | test_acc: 0.0000\nEpoch: 132 | train_loss: 0.0209 | train_acc: 0.9932 | test_loss: 155476.5938 | test_acc: 0.0000\nEpoch: 133 | train_loss: 0.0308 | train_acc: 0.9901 | test_loss: 20561.0664 | test_acc: 0.0000\nEpoch: 134 | train_loss: 0.0247 | train_acc: 0.9916 | test_loss: 85924.9766 | test_acc: 0.0000\nEpoch: 135 | train_loss: 0.0285 | train_acc: 0.9891 | test_loss: 40298.5195 | test_acc: 0.0000\nEpoch: 136 | train_loss: 0.0338 | train_acc: 0.9891 | test_loss: 34004.2734 | test_acc: 0.0000\nEpoch: 137 | train_loss: 0.0230 | train_acc: 0.9921 | test_loss: 19362.2148 | test_acc: 0.0000\nEpoch: 138 | train_loss: 0.0218 | train_acc: 0.9935 | test_loss: 180410.4219 | test_acc: 0.0000\nEpoch: 139 | train_loss: 0.0207 | train_acc: 0.9922 | test_loss: 2.7616 | test_acc: 0.5000\nEpoch: 140 | train_loss: 0.0282 | train_acc: 0.9911 | test_loss: 5494.1870 | test_acc: 0.0000\nEpoch: 141 | train_loss: 0.0305 | train_acc: 0.9904 | test_loss: 20482.1641 | test_acc: 0.0000\nEpoch: 142 | train_loss: 0.0271 | train_acc: 0.9917 | test_loss: 23762.6270 | test_acc: 0.0000\nEpoch: 143 | train_loss: 0.0221 | train_acc: 0.9927 | test_loss: 231052.8750 | test_acc: 0.0000\nEpoch: 144 | train_loss: 0.0273 | train_acc: 0.9914 | test_loss: 29707.2129 | test_acc: 0.0000\nEpoch: 145 | train_loss: 0.0241 | train_acc: 0.9908 | test_loss: 80442.2109 | test_acc: 0.0000\nEpoch: 146 | train_loss: 0.0264 | train_acc: 0.9904 | test_loss: 136314.0938 | test_acc: 0.0000\nEpoch: 147 | train_loss: 0.0338 | train_acc: 0.9890 | test_loss: 2.7027 | test_acc: 0.5000\nEpoch: 148 | train_loss: 0.0203 | train_acc: 0.9943 | test_loss: 4090.8442 | test_acc: 0.0000\nEpoch: 149 | train_loss: 0.0310 | train_acc: 0.9891 | test_loss: 144024.7500 | test_acc: 0.0000\nEpoch: 150 | train_loss: 0.0267 | train_acc: 0.9911 | test_loss: 212186.7188 | test_acc: 0.0000\nEpoch: 151 | train_loss: 0.0224 | train_acc: 0.9906 | test_loss: 24464.3574 | test_acc: 0.0000\nEpoch: 152 | train_loss: 0.0128 | train_acc: 0.9953 | test_loss: 170423.7812 | test_acc: 0.0000\nEpoch: 153 | train_loss: 0.0214 | train_acc: 0.9919 | test_loss: 142176.9688 | test_acc: 0.0000\nEpoch: 154 | train_loss: 0.0231 | train_acc: 0.9927 | test_loss: 87018.5234 | test_acc: 0.0000\nEpoch: 155 | train_loss: 0.0284 | train_acc: 0.9896 | test_loss: 308.0113 | test_acc: 0.0000\nEpoch: 156 | train_loss: 0.0187 | train_acc: 0.9945 | test_loss: 2.7418 | test_acc: 0.5000\nEpoch: 157 | train_loss: 0.0222 | train_acc: 0.9922 | test_loss: 270639.0625 | test_acc: 0.0000\nEpoch: 158 | train_loss: 0.0240 | train_acc: 0.9922 | test_loss: 6250.8120 | test_acc: 0.0000\nEpoch: 159 | train_loss: 0.0168 | train_acc: 0.9940 | test_loss: 15779.0830 | test_acc: 0.0000\nEpoch: 160 | train_loss: 0.0275 | train_acc: 0.9917 | test_loss: 15205.3975 | test_acc: 0.0000\nEpoch: 161 | train_loss: 0.0181 | train_acc: 0.9945 | test_loss: 130601.8359 | test_acc: 0.0000\nEpoch: 162 | train_loss: 0.0261 | train_acc: 0.9911 | test_loss: 113362.7109 | test_acc: 0.0000\nEpoch: 163 | train_loss: 0.0194 | train_acc: 0.9938 | test_loss: 128378.7500 | test_acc: 0.0000\nEpoch: 164 | train_loss: 0.0165 | train_acc: 0.9943 | test_loss: 116741.5469 | test_acc: 0.0000\nEpoch: 165 | train_loss: 0.0187 | train_acc: 0.9945 | test_loss: 41426.1328 | test_acc: 0.0000\nEpoch: 166 | train_loss: 0.0160 | train_acc: 0.9951 | test_loss: 1740.1810 | test_acc: 0.0000\nEpoch: 167 | train_loss: 0.0122 | train_acc: 0.9958 | test_loss: 197576.3750 | test_acc: 0.0000\nEpoch: 168 | train_loss: 0.0180 | train_acc: 0.9951 | test_loss: 35001.5703 | test_acc: 0.0000\nEpoch: 169 | train_loss: 0.0161 | train_acc: 0.9937 | test_loss: 10372.7598 | test_acc: 0.0000\nEpoch: 170 | train_loss: 0.0232 | train_acc: 0.9919 | test_loss: 159066.2500 | test_acc: 0.0000\nEpoch: 171 | train_loss: 0.0268 | train_acc: 0.9900 | test_loss: 52099.4141 | test_acc: 0.0000\nEpoch: 172 | train_loss: 0.0158 | train_acc: 0.9948 | test_loss: 34235.9180 | test_acc: 0.0000\nEpoch: 173 | train_loss: 0.0226 | train_acc: 0.9930 | test_loss: 74557.7500 | test_acc: 0.0000\nEpoch: 174 | train_loss: 0.0173 | train_acc: 0.9938 | test_loss: 351460.8750 | test_acc: 0.0000\nEpoch: 175 | train_loss: 0.0211 | train_acc: 0.9938 | test_loss: 109826.8438 | test_acc: 0.0000\nEpoch: 176 | train_loss: 0.0194 | train_acc: 0.9943 | test_loss: 16799.2168 | test_acc: 0.0000\nEpoch: 177 | train_loss: 0.0222 | train_acc: 0.9914 | test_loss: 3.0706 | test_acc: 0.5000\nEpoch: 178 | train_loss: 0.0198 | train_acc: 0.9932 | test_loss: 79166.7344 | test_acc: 0.0000\nEpoch: 179 | train_loss: 0.0192 | train_acc: 0.9948 | test_loss: 4.2905 | test_acc: 0.5000\nEpoch: 180 | train_loss: 0.0156 | train_acc: 0.9935 | test_loss: 38584.4609 | test_acc: 0.0000\nEpoch: 181 | train_loss: 0.0216 | train_acc: 0.9938 | test_loss: 3.5785 | test_acc: 0.5000\nEpoch: 182 | train_loss: 0.0220 | train_acc: 0.9932 | test_loss: 231962.9062 | test_acc: 0.0000\nEpoch: 183 | train_loss: 0.0185 | train_acc: 0.9961 | test_loss: 6045.9058 | test_acc: 0.0000\nEpoch: 184 | train_loss: 0.0152 | train_acc: 0.9935 | test_loss: 2958.2837 | test_acc: 0.0000\nEpoch: 185 | train_loss: 0.0167 | train_acc: 0.9932 | test_loss: 14459.5811 | test_acc: 0.0000\nEpoch: 186 | train_loss: 0.0128 | train_acc: 0.9961 | test_loss: 39827.8828 | test_acc: 0.0000\nEpoch: 187 | train_loss: 0.0165 | train_acc: 0.9940 | test_loss: 65788.1641 | test_acc: 0.0000\nEpoch: 188 | train_loss: 0.0164 | train_acc: 0.9945 | test_loss: 19896.8242 | test_acc: 0.0000\nEpoch: 189 | train_loss: 0.0141 | train_acc: 0.9956 | test_loss: 162443.2656 | test_acc: 0.0000\nEpoch: 190 | train_loss: 0.0206 | train_acc: 0.9938 | test_loss: 14859.6729 | test_acc: 0.0000\nEpoch: 191 | train_loss: 0.0109 | train_acc: 0.9966 | test_loss: 223074.7969 | test_acc: 0.0000\nEpoch: 192 | train_loss: 0.0142 | train_acc: 0.9956 | test_loss: 19944.8047 | test_acc: 0.0000\nEpoch: 193 | train_loss: 0.0138 | train_acc: 0.9966 | test_loss: 137623.7344 | test_acc: 0.0000\nEpoch: 194 | train_loss: 0.0257 | train_acc: 0.9924 | test_loss: 300350.5312 | test_acc: 0.0000\nEpoch: 195 | train_loss: 0.0205 | train_acc: 0.9921 | test_loss: 14778.8877 | test_acc: 0.0000\nEpoch: 196 | train_loss: 0.0161 | train_acc: 0.9945 | test_loss: 3.9783 | test_acc: 0.5000\nEpoch: 197 | train_loss: 0.0168 | train_acc: 0.9958 | test_loss: 41133.3906 | test_acc: 0.0000\nEpoch: 198 | train_loss: 0.0165 | train_acc: 0.9951 | test_loss: 23891.9258 | test_acc: 0.0000\nEpoch: 199 | train_loss: 0.0170 | train_acc: 0.9943 | test_loss: 38.0321 | test_acc: 0.0000\nEpoch: 200 | train_loss: 0.0121 | train_acc: 0.9958 | test_loss: 3.2367 | test_acc: 0.5000\nEpoch: 201 | train_loss: 0.0140 | train_acc: 0.9945 | test_loss: 3.0667 | test_acc: 0.5000\nEpoch: 202 | train_loss: 0.0133 | train_acc: 0.9945 | test_loss: 107558.8984 | test_acc: 0.0000\nEpoch: 203 | train_loss: 0.0122 | train_acc: 0.9966 | test_loss: 17751.7383 | test_acc: 0.0000\nEpoch: 204 | train_loss: 0.0163 | train_acc: 0.9956 | test_loss: 51229.8828 | test_acc: 0.0000\nEpoch: 205 | train_loss: 0.0193 | train_acc: 0.9940 | test_loss: 286533.3750 | test_acc: 0.0000\nEpoch: 206 | train_loss: 0.0207 | train_acc: 0.9919 | test_loss: 3.3862 | test_acc: 0.5000\nEpoch: 207 | train_loss: 0.0168 | train_acc: 0.9926 | test_loss: 125364.3203 | test_acc: 0.0000\nEpoch: 208 | train_loss: 0.0139 | train_acc: 0.9948 | test_loss: 13747.7178 | test_acc: 0.0000\nEpoch: 209 | train_loss: 0.0193 | train_acc: 0.9929 | test_loss: 20963.2148 | test_acc: 0.0000\nEpoch: 210 | train_loss: 0.0156 | train_acc: 0.9943 | test_loss: 33865.8828 | test_acc: 0.0000\nEpoch: 211 | train_loss: 0.0134 | train_acc: 0.9958 | test_loss: 18619.0547 | test_acc: 0.0000\nEpoch: 212 | train_loss: 0.0166 | train_acc: 0.9938 | test_loss: 41917.6680 | test_acc: 0.0000\nEpoch: 213 | train_loss: 0.0229 | train_acc: 0.9930 | test_loss: 29051.0645 | test_acc: 0.0000\nEpoch: 214 | train_loss: 0.0207 | train_acc: 0.9945 | test_loss: 69247.0234 | test_acc: 0.0000\nEpoch: 215 | train_loss: 0.0191 | train_acc: 0.9942 | test_loss: 2.3494 | test_acc: 0.5000\nEpoch: 216 | train_loss: 0.0180 | train_acc: 0.9932 | test_loss: 3.3917 | test_acc: 0.5000\nEpoch: 217 | train_loss: 0.0236 | train_acc: 0.9914 | test_loss: 24453.5391 | test_acc: 0.0000\nEpoch: 218 | train_loss: 0.0116 | train_acc: 0.9969 | test_loss: 98467.9297 | test_acc: 0.0000\nEpoch: 219 | train_loss: 0.0133 | train_acc: 0.9958 | test_loss: 27584.2871 | test_acc: 0.0000\nEpoch: 220 | train_loss: 0.0139 | train_acc: 0.9947 | test_loss: 145479.8594 | test_acc: 0.0000\nEpoch: 221 | train_loss: 0.0197 | train_acc: 0.9930 | test_loss: 15877.7295 | test_acc: 0.0000\nEpoch: 222 | train_loss: 0.0137 | train_acc: 0.9955 | test_loss: 210202.0625 | test_acc: 0.0000\nEpoch: 223 | train_loss: 0.0141 | train_acc: 0.9958 | test_loss: 17897.4902 | test_acc: 0.0000\nEpoch: 224 | train_loss: 0.0131 | train_acc: 0.9948 | test_loss: 2.8836 | test_acc: 0.5000\nEpoch: 225 | train_loss: 0.0126 | train_acc: 0.9962 | test_loss: 12366.8301 | test_acc: 0.0000\nEpoch: 226 | train_loss: 0.0085 | train_acc: 0.9966 | test_loss: 19421.1426 | test_acc: 0.0000\nEpoch: 227 | train_loss: 0.0146 | train_acc: 0.9953 | test_loss: 12534.3730 | test_acc: 0.0000\nEpoch: 228 | train_loss: 0.0203 | train_acc: 0.9938 | test_loss: 13212.1875 | test_acc: 0.0000\nEpoch: 229 | train_loss: 0.0197 | train_acc: 0.9940 | test_loss: 3414.9832 | test_acc: 0.0000\nEpoch: 230 | train_loss: 0.0122 | train_acc: 0.9966 | test_loss: 27300.8027 | test_acc: 0.0000\nEpoch: 231 | train_loss: 0.0076 | train_acc: 0.9977 | test_loss: 89041.2969 | test_acc: 0.0000\nEpoch: 232 | train_loss: 0.0100 | train_acc: 0.9961 | test_loss: 178092.5156 | test_acc: 0.0000\nEpoch: 233 | train_loss: 0.0134 | train_acc: 0.9964 | test_loss: 21591.1582 | test_acc: 0.0000\nEpoch: 234 | train_loss: 0.0142 | train_acc: 0.9945 | test_loss: 331467.7188 | test_acc: 0.0000\nEpoch: 235 | train_loss: 0.0098 | train_acc: 0.9966 | test_loss: 88295.8672 | test_acc: 0.0000\nEpoch: 236 | train_loss: 0.0103 | train_acc: 0.9977 | test_loss: 55621.3164 | test_acc: 0.0000\nEpoch: 237 | train_loss: 0.0091 | train_acc: 0.9966 | test_loss: 24295.6387 | test_acc: 0.0000\nEpoch: 238 | train_loss: 0.0091 | train_acc: 0.9969 | test_loss: 6558.3994 | test_acc: 0.0000\nEpoch: 239 | train_loss: 0.0124 | train_acc: 0.9961 | test_loss: 224104.7812 | test_acc: 0.0000\nEpoch: 240 | train_loss: 0.0125 | train_acc: 0.9958 | test_loss: 4.0401 | test_acc: 0.5000\nEpoch: 241 | train_loss: 0.0132 | train_acc: 0.9961 | test_loss: 17399.8613 | test_acc: 0.0000\nEpoch: 242 | train_loss: 0.0107 | train_acc: 0.9958 | test_loss: 10310.2666 | test_acc: 0.0000\nEpoch: 243 | train_loss: 0.0147 | train_acc: 0.9951 | test_loss: 46483.9375 | test_acc: 0.0000\nEpoch: 244 | train_loss: 0.0077 | train_acc: 0.9982 | test_loss: 48845.8633 | test_acc: 0.0000\nEpoch: 245 | train_loss: 0.0191 | train_acc: 0.9945 | test_loss: 21855.7852 | test_acc: 0.0000\nEpoch: 246 | train_loss: 0.0081 | train_acc: 0.9984 | test_loss: 8584.4785 | test_acc: 0.0000\nEpoch: 247 | train_loss: 0.0145 | train_acc: 0.9956 | test_loss: 112740.8594 | test_acc: 0.0000\nEpoch: 248 | train_loss: 0.0121 | train_acc: 0.9953 | test_loss: 8942.6045 | test_acc: 0.0000\nEpoch: 249 | train_loss: 0.0103 | train_acc: 0.9971 | test_loss: 94462.4062 | test_acc: 0.0000\nEpoch: 250 | train_loss: 0.0120 | train_acc: 0.9953 | test_loss: 59502.2891 | test_acc: 0.0000\nEpoch: 251 | train_loss: 0.0107 | train_acc: 0.9966 | test_loss: 21100.0859 | test_acc: 0.0000\nEpoch: 252 | train_loss: 0.0075 | train_acc: 0.9974 | test_loss: 21676.8594 | test_acc: 0.0000\nEpoch: 253 | train_loss: 0.0084 | train_acc: 0.9969 | test_loss: 17144.0000 | test_acc: 0.0000\nEpoch: 254 | train_loss: 0.0128 | train_acc: 0.9969 | test_loss: 13551.6025 | test_acc: 0.0000\nEpoch: 255 | train_loss: 0.0134 | train_acc: 0.9951 | test_loss: 57931.7031 | test_acc: 0.0000\nEpoch: 256 | train_loss: 0.0085 | train_acc: 0.9977 | test_loss: 82175.0469 | test_acc: 0.0000\nEpoch: 257 | train_loss: 0.0091 | train_acc: 0.9971 | test_loss: 121533.2500 | test_acc: 0.0000\nEpoch: 258 | train_loss: 0.0074 | train_acc: 0.9987 | test_loss: 10773.0547 | test_acc: 0.0000\nEpoch: 259 | train_loss: 0.0182 | train_acc: 0.9935 | test_loss: 79606.0859 | test_acc: 0.0000\nEpoch: 260 | train_loss: 0.0133 | train_acc: 0.9951 | test_loss: 33123.2695 | test_acc: 0.0000\nEpoch: 261 | train_loss: 0.0097 | train_acc: 0.9964 | test_loss: 20017.6289 | test_acc: 0.0000\nEpoch: 262 | train_loss: 0.0138 | train_acc: 0.9958 | test_loss: 12747.5322 | test_acc: 0.0000\nEpoch: 263 | train_loss: 0.0129 | train_acc: 0.9956 | test_loss: 190869.0312 | test_acc: 0.0000\nEpoch: 264 | train_loss: 0.0138 | train_acc: 0.9951 | test_loss: 132807.1875 | test_acc: 0.0000\nEpoch: 265 | train_loss: 0.0098 | train_acc: 0.9969 | test_loss: 17867.8828 | test_acc: 0.0000\nEpoch: 266 | train_loss: 0.0090 | train_acc: 0.9979 | test_loss: 34213.0586 | test_acc: 0.0000\nEpoch: 267 | train_loss: 0.0109 | train_acc: 0.9958 | test_loss: 239.8985 | test_acc: 0.0000\nEpoch: 268 | train_loss: 0.0065 | train_acc: 0.9979 | test_loss: 145760.3906 | test_acc: 0.0000\nEpoch: 269 | train_loss: 0.0081 | train_acc: 0.9971 | test_loss: 3.2943 | test_acc: 0.5000\nEpoch: 270 | train_loss: 0.0096 | train_acc: 0.9969 | test_loss: 37508.4102 | test_acc: 0.0000\nEpoch: 271 | train_loss: 0.0101 | train_acc: 0.9966 | test_loss: 91315.2812 | test_acc: 0.0000\nEpoch: 272 | train_loss: 0.0132 | train_acc: 0.9955 | test_loss: 92735.3438 | test_acc: 0.0000\nEpoch: 273 | train_loss: 0.0088 | train_acc: 0.9971 | test_loss: 50701.5898 | test_acc: 0.0000\nEpoch: 274 | train_loss: 0.0100 | train_acc: 0.9966 | test_loss: 103397.3203 | test_acc: 0.0000\nEpoch: 275 | train_loss: 0.0132 | train_acc: 0.9956 | test_loss: 132001.1406 | test_acc: 0.0000\nEpoch: 276 | train_loss: 0.0091 | train_acc: 0.9961 | test_loss: 27409.7715 | test_acc: 0.0000\nEpoch: 277 | train_loss: 0.0107 | train_acc: 0.9961 | test_loss: 13889.3428 | test_acc: 0.0000\nEpoch: 278 | train_loss: 0.0089 | train_acc: 0.9984 | test_loss: 3.1569 | test_acc: 0.5000\nEpoch: 279 | train_loss: 0.0119 | train_acc: 0.9964 | test_loss: 151906.6094 | test_acc: 0.0000\nEpoch: 280 | train_loss: 0.0075 | train_acc: 0.9977 | test_loss: 21906.4512 | test_acc: 0.0000\nEpoch: 281 | train_loss: 0.0117 | train_acc: 0.9963 | test_loss: 140660.8281 | test_acc: 0.0000\nEpoch: 282 | train_loss: 0.0123 | train_acc: 0.9961 | test_loss: 115299.6719 | test_acc: 0.0000\nEpoch: 283 | train_loss: 0.0154 | train_acc: 0.9953 | test_loss: 140321.1094 | test_acc: 0.0000\nEpoch: 284 | train_loss: 0.0115 | train_acc: 0.9956 | test_loss: 167150.0469 | test_acc: 0.0000\nEpoch: 285 | train_loss: 0.0064 | train_acc: 0.9979 | test_loss: 40529.0781 | test_acc: 0.0000\nEpoch: 286 | train_loss: 0.0111 | train_acc: 0.9964 | test_loss: 18602.7734 | test_acc: 0.0000\nEpoch: 287 | train_loss: 0.0108 | train_acc: 0.9958 | test_loss: 2.4874 | test_acc: 0.5000\nEpoch: 288 | train_loss: 0.0116 | train_acc: 0.9963 | test_loss: 396180.4375 | test_acc: 0.0000\nEpoch: 289 | train_loss: 0.0089 | train_acc: 0.9969 | test_loss: 78424.7266 | test_acc: 0.0000\nEpoch: 290 | train_loss: 0.0044 | train_acc: 0.9982 | test_loss: 21592.3613 | test_acc: 0.0000\nEpoch: 291 | train_loss: 0.0125 | train_acc: 0.9963 | test_loss: 65537.6094 | test_acc: 0.0000\nEpoch: 292 | train_loss: 0.0127 | train_acc: 0.9953 | test_loss: 2.7487 | test_acc: 0.5000\nEpoch: 293 | train_loss: 0.0116 | train_acc: 0.9966 | test_loss: 92718.8984 | test_acc: 0.0000\nEpoch: 294 | train_loss: 0.0082 | train_acc: 0.9974 | test_loss: 19523.5234 | test_acc: 0.0000\nEpoch: 295 | train_loss: 0.0122 | train_acc: 0.9964 | test_loss: 87752.9688 | test_acc: 0.0000\nEpoch: 296 | train_loss: 0.0090 | train_acc: 0.9974 | test_loss: 21325.4707 | test_acc: 0.0000\nEpoch: 297 | train_loss: 0.0075 | train_acc: 0.9979 | test_loss: 38218.8203 | test_acc: 0.0000\nEpoch: 298 | train_loss: 0.0117 | train_acc: 0.9964 | test_loss: 24709.5586 | test_acc: 0.0000\nEpoch: 299 | train_loss: 0.0076 | train_acc: 0.9966 | test_loss: 81204.6719 | test_acc: 0.0000\nEpoch: 300 | train_loss: 0.0107 | train_acc: 0.9977 | test_loss: 96087.5781 | test_acc: 0.0000\nEpoch: 301 | train_loss: 0.0126 | train_acc: 0.9953 | test_loss: 16144.6533 | test_acc: 0.0000\nEpoch: 302 | train_loss: 0.0095 | train_acc: 0.9971 | test_loss: 41605.3594 | test_acc: 0.0000\nEpoch: 303 | train_loss: 0.0182 | train_acc: 0.9953 | test_loss: 23104.5684 | test_acc: 0.0000\nEpoch: 304 | train_loss: 0.0109 | train_acc: 0.9964 | test_loss: 140455.1094 | test_acc: 0.0000\nEpoch: 305 | train_loss: 0.0164 | train_acc: 0.9948 | test_loss: 216610.6406 | test_acc: 0.0000\nEpoch: 306 | train_loss: 0.0100 | train_acc: 0.9969 | test_loss: 150105.2500 | test_acc: 0.0000\nEpoch: 307 | train_loss: 0.0075 | train_acc: 0.9974 | test_loss: 46585.9141 | test_acc: 0.0000\nEpoch: 308 | train_loss: 0.0059 | train_acc: 0.9981 | test_loss: 158841.9688 | test_acc: 0.0000\nEpoch: 309 | train_loss: 0.0104 | train_acc: 0.9969 | test_loss: 121498.6875 | test_acc: 0.0000\nEpoch: 310 | train_loss: 0.0159 | train_acc: 0.9940 | test_loss: 194942.0625 | test_acc: 0.0000\nEpoch: 311 | train_loss: 0.0084 | train_acc: 0.9966 | test_loss: 98655.3906 | test_acc: 0.0000\nEpoch: 312 | train_loss: 0.0114 | train_acc: 0.9958 | test_loss: 35002.4805 | test_acc: 0.0000\nEpoch: 313 | train_loss: 0.0194 | train_acc: 0.9948 | test_loss: 28565.5762 | test_acc: 0.0000\nEpoch: 314 | train_loss: 0.0075 | train_acc: 0.9979 | test_loss: 3.0230 | test_acc: 0.5000\nEpoch: 315 | train_loss: 0.0082 | train_acc: 0.9977 | test_loss: 59418.0938 | test_acc: 0.0000\nEpoch: 316 | train_loss: 0.0069 | train_acc: 0.9982 | test_loss: 204244.8906 | test_acc: 0.0000\nEpoch: 317 | train_loss: 0.0069 | train_acc: 0.9973 | test_loss: 24923.5176 | test_acc: 0.0000\nEpoch: 318 | train_loss: 0.0069 | train_acc: 0.9979 | test_loss: 52703.1016 | test_acc: 0.0000\nEpoch: 319 | train_loss: 0.0118 | train_acc: 0.9966 | test_loss: 247001.5000 | test_acc: 0.0000\nEpoch: 320 | train_loss: 0.0079 | train_acc: 0.9974 | test_loss: 2.7655 | test_acc: 0.5000\nEpoch: 321 | train_loss: 0.0085 | train_acc: 0.9966 | test_loss: 39279.8633 | test_acc: 0.0000\nEpoch: 322 | train_loss: 0.0088 | train_acc: 0.9977 | test_loss: 21511.5742 | test_acc: 0.0000\nEpoch: 323 | train_loss: 0.0113 | train_acc: 0.9971 | test_loss: 139479.9375 | test_acc: 0.0000\nEpoch: 324 | train_loss: 0.0089 | train_acc: 0.9974 | test_loss: 12051.6387 | test_acc: 0.0000\nEpoch: 325 | train_loss: 0.0069 | train_acc: 0.9977 | test_loss: 17540.0312 | test_acc: 0.0000\nEpoch: 326 | train_loss: 0.0079 | train_acc: 0.9969 | test_loss: 34835.8320 | test_acc: 0.0000\nEpoch: 327 | train_loss: 0.0056 | train_acc: 0.9982 | test_loss: 42936.4727 | test_acc: 0.0000\nEpoch: 328 | train_loss: 0.0071 | train_acc: 0.9966 | test_loss: 162275.7188 | test_acc: 0.0000\nEpoch: 329 | train_loss: 0.0120 | train_acc: 0.9964 | test_loss: 16912.4141 | test_acc: 0.0000\nEpoch: 330 | train_loss: 0.0062 | train_acc: 0.9977 | test_loss: 112166.3594 | test_acc: 0.0000\nEpoch: 331 | train_loss: 0.0122 | train_acc: 0.9964 | test_loss: 54380.2930 | test_acc: 0.0000\nEpoch: 332 | train_loss: 0.0099 | train_acc: 0.9960 | test_loss: 730092.6875 | test_acc: 0.0000\nEpoch: 333 | train_loss: 0.0072 | train_acc: 0.9971 | test_loss: 251058.7344 | test_acc: 0.0000\nEpoch: 334 | train_loss: 0.0047 | train_acc: 0.9987 | test_loss: 92609.2031 | test_acc: 0.0000\nEpoch: 335 | train_loss: 0.0071 | train_acc: 0.9979 | test_loss: 34937.1016 | test_acc: 0.0000\nEpoch: 336 | train_loss: 0.0085 | train_acc: 0.9977 | test_loss: 161901.3281 | test_acc: 0.0000\nEpoch: 337 | train_loss: 0.0077 | train_acc: 0.9979 | test_loss: 22418.2559 | test_acc: 0.0000\nEpoch: 338 | train_loss: 0.0094 | train_acc: 0.9971 | test_loss: 326929.5312 | test_acc: 0.0000\nEpoch: 339 | train_loss: 0.0082 | train_acc: 0.9966 | test_loss: 415081.8438 | test_acc: 0.0000\nEpoch: 340 | train_loss: 0.0108 | train_acc: 0.9964 | test_loss: 87.7362 | test_acc: 0.0000\nEpoch: 341 | train_loss: 0.0090 | train_acc: 0.9966 | test_loss: 150299.3125 | test_acc: 0.0000\nEpoch: 342 | train_loss: 0.0069 | train_acc: 0.9974 | test_loss: 3.4866 | test_acc: 0.5000\nEpoch: 343 | train_loss: 0.0051 | train_acc: 0.9982 | test_loss: 156628.2500 | test_acc: 0.0000\nEpoch: 344 | train_loss: 0.0084 | train_acc: 0.9974 | test_loss: 140311.6406 | test_acc: 0.0000\nEpoch: 345 | train_loss: 0.0088 | train_acc: 0.9966 | test_loss: 123839.5781 | test_acc: 0.0000\nEpoch: 346 | train_loss: 0.0103 | train_acc: 0.9966 | test_loss: 52760.0664 | test_acc: 0.0000\nEpoch: 347 | train_loss: 0.0102 | train_acc: 0.9961 | test_loss: 74852.3672 | test_acc: 0.0000\nEpoch: 348 | train_loss: 0.0080 | train_acc: 0.9966 | test_loss: 3.6692 | test_acc: 0.5000\nEpoch: 349 | train_loss: 0.0052 | train_acc: 0.9984 | test_loss: 124255.8516 | test_acc: 0.0000\nEpoch: 350 | train_loss: 0.0120 | train_acc: 0.9979 | test_loss: 42824.4961 | test_acc: 0.0000\nEpoch: 351 | train_loss: 0.0090 | train_acc: 0.9966 | test_loss: 611463.6875 | test_acc: 0.0000\nEpoch: 352 | train_loss: 0.0077 | train_acc: 0.9974 | test_loss: 119565.1953 | test_acc: 0.0000\nEpoch: 353 | train_loss: 0.0115 | train_acc: 0.9977 | test_loss: 3.4143 | test_acc: 0.5000\nEpoch: 354 | train_loss: 0.0104 | train_acc: 0.9971 | test_loss: 3.7167 | test_acc: 0.5000\nEpoch: 355 | train_loss: 0.0060 | train_acc: 0.9974 | test_loss: 18130.6758 | test_acc: 0.0000\nEpoch: 356 | train_loss: 0.0066 | train_acc: 0.9971 | test_loss: 21318.8535 | test_acc: 0.0000\nEpoch: 357 | train_loss: 0.0080 | train_acc: 0.9966 | test_loss: 232714.7344 | test_acc: 0.0000\nEpoch: 358 | train_loss: 0.0031 | train_acc: 0.9990 | test_loss: 29281.8691 | test_acc: 0.0000\nEpoch: 359 | train_loss: 0.0065 | train_acc: 0.9982 | test_loss: 260.1742 | test_acc: 0.0000\nEpoch: 360 | train_loss: 0.0079 | train_acc: 0.9971 | test_loss: 19147.3496 | test_acc: 0.0000\nEpoch: 361 | train_loss: 0.0079 | train_acc: 0.9974 | test_loss: 21477.1055 | test_acc: 0.0000\nEpoch: 362 | train_loss: 0.0098 | train_acc: 0.9977 | test_loss: 54939.2773 | test_acc: 0.0000\nEpoch: 363 | train_loss: 0.0108 | train_acc: 0.9966 | test_loss: 141258.9688 | test_acc: 0.0000\nEpoch: 364 | train_loss: 0.0096 | train_acc: 0.9969 | test_loss: 4851.5879 | test_acc: 0.0000\nEpoch: 365 | train_loss: 0.0062 | train_acc: 0.9971 | test_loss: 45183.6406 | test_acc: 0.0000\nEpoch: 366 | train_loss: 0.0068 | train_acc: 0.9974 | test_loss: 172174.8594 | test_acc: 0.0000\nEpoch: 367 | train_loss: 0.0043 | train_acc: 0.9987 | test_loss: 70398.4141 | test_acc: 0.0000\nEpoch: 368 | train_loss: 0.0071 | train_acc: 0.9982 | test_loss: 118641.6953 | test_acc: 0.0000\nEpoch: 369 | train_loss: 0.0092 | train_acc: 0.9977 | test_loss: 287902.6250 | test_acc: 0.0000\n[INFO] Total training time: 17340.952 seconds\nCPU times: user 1h 44min 22s, sys: 13min 39s, total: 1h 58min 2s\nWall time: 4h 49min\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the trained model to a file\ntorch.save(model, 'Epoch: 369 | train_loss: 0.0044 | train_acc: 0.9982 | test_loss: 21592.3613 | test_acc: 0.0000.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-31T10:57:23.965206Z","iopub.execute_input":"2024-05-31T10:57:23.965602Z","iopub.status.idle":"2024-05-31T10:57:24.262862Z","shell.execute_reply.started":"2024-05-31T10:57:23.965571Z","shell.execute_reply":"2024-05-31T10:57:24.261762Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}